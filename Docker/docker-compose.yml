version: '3.8'
services:
  tfserver:
    image: tensorflow/serving:latest-gpu
    hostname: tfserver
    container_name: tfserver
    restart: always
     #command: "tensorflow_model_server --port=8500 --grpc_socket_path=8500 --grpc_max_threads=2 --rest_api_port=8501 --model_name=tracknet_trt --model_base_path=/models/"
    networks:
      - inference
    ports:
      - "8500:8500"
      - "8501:8501"
    environment:
      MODEL_NAME: "tracknet_trt"
    volumes:
      - models:/models/
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]


#! VOLUMES
volumes:
  models:
    driver_opts:
      type: "nfs"
      o: "nfsvers=4,addr=192.168.1.120,rw,nolock,soft"
      device: ":/InferenceServerModels/TFServing/models"


#! NETWORK
networks:
  inference:
    driver: bridge
